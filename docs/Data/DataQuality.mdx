---
title: "Data Quality"
---

Driven by Product & Stakeholders e.S. NOW

Reliance on manual Intervention or flags ho id a quality issue

→ Set expectations

→ Producers set expectations When creating a feed (Feedr2)

- Build into Feedu2
- Not a key stakeholder for delivery but they will help deliver the solution

→ Primary benefit is for Consumers

## DQ Domains

six Dimensions

- Consistency
- Accuracy
- Validity
- Completeness
- Timeliness
- Uniqueness

### DQ Domains
- Standardisation
- Transformation
- Retry
- Accept duplicates rather than risk losing data

### What do we have already?

- There are DQ checks Which are stored in a table in BQ

### What are we lacking?

- No unified view - no Cohesive Ul
- What are the Data Quality rules against a table ?
- Data Quality Rule Engine
    - maybe want other people to define their an checks - What do Data Scientists Check for to decide a dataset is usable?

### Tods considered

- Fully self-build

- Great Expectations

- Anamalo - supports different Cloud environments

- Monte carlo

- Acryl Datahub

- Google Dataplex → Chosen

- AWS Glue DQ

## Strategy

- Tabs in BigQuery for Data Quality are supplied by Dataplex
- security don't like the data canyout cho a seperate 3rd party tool so we would have to build our even clusters etc.
- Dataplex keeps this in hause, so we pay for the computing resourcing No immediate need to pay for slots
- Could build our own but this is a short tom
- Datathatb → potentially a link to the Data Tab in Big Query. Plan integration.
- Testing out with incidents to show what a Data Quality would
- have caught
- Gap is around automated or ML based Anomalies